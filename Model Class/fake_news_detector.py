# -*- coding: utf-8 -*-
"""fake_news_detector.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1W3ak2eYsNuPoXklqSy2G_PmAdMMzKy1F
"""

import pandas as pd
import numpy as np
import tensorflow as tf
import pickle
import json


class PreProcessor:
  def __init__(self):
    pass

  def combine_text(self,title,content):
    return "tle "+title+" txt "+content

  def remove_number(self,txt):
    result=""
    words=txt.split()
    for word in words:
      if word.isnumeric() == True:
        result += "10 "
      else:
        result+=word
        result+=" "
    return result.strip()

  def remove_punct(self,test_str):
    punc = '''!()-[]{};:'"\,‘’<>./?@#$%^&*_~“”'''
    for ele in test_str:
        if ele in punc:
            test_str = test_str.replace(ele, "")
    return test_str

import pandas as pd
import numpy as np
import tensorflow as tf
import pickle
import json

tokenizer_path = "/content/drive/MyDrive/Dataset/tokenizer_ann.pickle"
model_path = "/content/drive/MyDrive/Dataset/ann_model.hdf5"

class FakeNewsDetector:
  def __init__(self):
    self.model = tf.keras.models.load_model(model_path)
    with open(tokenizer_path,"rb") as fp:
      self.tokenizer = pickle.load(fp)
    self.preprocessor = PreProcessor()

  def preprocess(self,title,content):
    combined_text = self.preprocessor.combine_text(title,content)
    combined_text = self.preprocessor.remove_number(combined_text)
    combined_text = self.preprocessor.remove_punct(combined_text)
    return combined_text

  def predict(self,title,content):
    text = self.preprocess(title,content)
    seq = self.tokenizer.texts_to_sequences([text])
    seq = tf.keras.preprocessing.sequence.pad_sequences(seq, maxlen=100)
    result = self.model.predict(seq)
    if result[0][0]>0.5:
      return "This news is True"
    else:
      return "This news is Fake"

